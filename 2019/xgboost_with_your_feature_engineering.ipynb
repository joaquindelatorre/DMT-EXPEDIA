{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"xgboost_with_your_feature_engineering.ipynb","provenance":[{"file_id":"1IPPYTBEpbQOyRj9qbtgyiUuLQzAgwyDi","timestamp":1590129170684},{"file_id":"1vG-CNfcb_DmeHKY4qcxdQI83YIJAB1De","timestamp":1590089008241}],"mount_file_id":"1IPPYTBEpbQOyRj9qbtgyiUuLQzAgwyDi","authorship_tag":"ABX9TyPE/ET2SQvctxychB+fgodj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"HRcjb-PNJbkl","colab_type":"code","outputId":"1569e4cd-1725-4578-eaa1-eb631eca2e44","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import time\n","import argparse\n","import pickle\n","import os\n","import gc\n","\n","import pandas\n","import numpy as np\n","import lightgbm\n","import xgboost as xgb\n","\n","\n","def load_data(file_path):\n","    gc.collect()\n","    print(\"Loading File {}\".format(file_path))\n","    data = pandas.read_csv(file_path)\n","    print(\"Finished loading data ....\")\n","    return data\n","\n","\n","def add_date_features(\n","    in_data, datetime_key=\"date_time\", features=[\"month\", \"hour\", \"dayofweek\"]\n","):\n","    dates = pandas.to_datetime(in_data[datetime_key])\n","    for feature in features:\n","        if feature == \"month\":\n","            in_data[\"month\"] = dates.dt.month\n","        elif feature == \"dayofweek\":\n","            in_data[\"dayofweek\"] = dates.dt.dayofweek\n","        elif feature == \"hour\":\n","            in_data[\"hour\"] = dates.dt.hour\n","\n","    return in_data\n","\n","\n","def normalize_features(input_df, group_key, target_column, take_log10=False):\n","\n","    # for numerical stability\n","    epsilon = 1e-4\n","    if take_log10:\n","        input_df[target_column] = np.log10(input_df[target_column] + epsilon)\n","    methods = [\"mean\", \"std\"]\n","\n","    df = input_df.groupby(group_key).agg({target_column: methods})\n","\n","    df.columns = df.columns.droplevel()\n","    col = {}\n","    for method in methods:\n","        col[method] = target_column + \"_\" + method\n","\n","    df.rename(columns=col, inplace=True)\n","    df_merge = input_df.merge(df.reset_index(), on=group_key)\n","    df_merge[target_column + \"_norm_by_\" + group_key] = (\n","        df_merge[target_column] - df_merge[target_column + \"_mean\"]\n","    ) / df_merge[target_column + \"_std\"]\n","    df_merge = df_merge.drop(labels=[col[\"mean\"], col[\"std\"]], axis=1)\n","\n","    gc.collect()\n","    return df_merge\n","\n","\n","# Add new columns to the dataframe\n","def aggregated_features_single_column(\n","    in_data,\n","    key_for_grouped_by=\"prop_id\",\n","    target_column=\"price_usd\",\n","    agg_methods=[\"mean\", \"median\", \"min\", \"max\"],\n","    transform_methods={\"mean\": [\"substract\"]},\n","):\n","    df = in_data.groupby(key_for_grouped_by).agg({target_column: agg_methods})\n","\n","    if isinstance(key_for_grouped_by, list):\n","        str_key_for_grouped_by = \"|\".join(key_for_grouped_by)\n","    else:\n","        str_key_for_grouped_by = key_for_grouped_by\n","\n","    df.columns = df.columns.droplevel()\n","    col = {}\n","    for method in agg_methods:\n","        col[method] = (\n","            method.upper() + \"(\" + str_key_for_grouped_by + \", \" + target_column + \")\"\n","        )\n","\n","    df.rename(columns=col, inplace=True)\n","\n","    in_data = in_data.merge(df.reset_index(), on=key_for_grouped_by)\n","    for method_name in transform_methods:\n","        for applying_function in transform_methods[method_name]:\n","            function_data = in_data[\n","                method_name.upper()\n","                + \"(\"\n","                + str_key_for_grouped_by\n","                + \", \"\n","                + target_column\n","                + \")\"\n","            ]\n","            column_data = in_data[target_column]\n","            if applying_function == \"substract\":\n","                result = column_data - function_data\n","            elif applying_function == \"divide\":\n","                result = column_data / function_data\n","            else:\n","                continue\n","            in_data[\n","                applying_function.upper()\n","                + \"(\"\n","                + target_column\n","                + \", \"\n","                + method_name.upper()\n","                + \")\"\n","            ] = result\n","    gc.collect()\n","\n","    return in_data\n","\n","\n","def drop_columns_with_missing_data(\n","    df,\n","    threshold,\n","    ignore_values=[\n","        \"visitor_hist_adr_usd\",\n","        \"visitor_hist_starrating\",\n","        \"srch_query_affinity_score\",\n","    ],\n","):\n","    columns_to_drop = []\n","\n","    for i in range(df.shape[1]):\n","        length_df = len(df)\n","        column_names = df.columns.tolist()\n","        number_nans = sum(df.iloc[:, i].isnull())\n","        if number_nans / length_df > threshold:\n","            if column_names[i] not in ignore_values:\n","                columns_to_drop.append(column_names[i])\n","\n","    print(\n","        \"Dropping columns {} because they miss more than {} ratio of data.\".format(\n","            columns_to_drop, threshold\n","        )\n","    )\n","\n","    df_reduced = df.drop(labels=columns_to_drop, axis=1)\n","    print(\"Dropped columns {}\".format(columns_to_drop))\n","    return df_reduced\n","\n","def add_quartile_location_data(data):\n","    loc_q = data.groupby(\"prop_country_id\")[\"prop_location_score2\"].quantile(q=0.25)\n","    data[\"prop_location_score2_quartile\"] = loc_q.reindex([data.prop_id]).values\n","    data[\"prop_location_score2\"].fillna(data[\"prop_location_score2_quartile\"])\n","    data.drop(labels=[\"prop_location_score2_quartile\"], axis=1, inplace=True)\n","    return data\n","\n","def preprocess_training_data(orig_data, kind=\"train\", use_ndcg_choices=False):\n","\n","    print(\"Preprocessing training data....\")\n","    gc.collect()\n","    data_for_training = orig_data\n","\n","    target_column = \"target\"\n","\n","    if kind == \"train\":\n","        conditions = [\n","            data_for_training[\"click_bool\"] == 1,\n","            data_for_training[\"booking_bool\"] == 1,\n","        ]\n","        choices = [1, 2]\n","        data_for_training[target_column] = np.select(conditions, choices, default=0)\n","\n","    threshold = 0.9\n","    data_for_training = drop_columns_with_missing_data(data_for_training, threshold)\n","    data_for_training = add_date_features(data_for_training)\n","    data_for_training.drop(labels=[\"date_time\"], axis=1, inplace=True)\n","    \n","\n","\n","    data_for_training = normalize_features(\n","        data_for_training,\n","        group_key=\"srch_id\",\n","        target_column=\"price_usd\",\n","        take_log10=True,\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"prop_id\", target_column=\"price_usd\"\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_starrating\"\n","    )\n","    #Normalize prop_location_score2:\n","    data_for_training = add_quartile_location_data(data_for_training)\n","\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training, \"prop_id\", \"price_usd\", [\"mean\"]\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"prop_starrating\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"prop_location_score2\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"prop_location_score1\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_destination_id\",\n","        target_column=\"price_usd\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"prop_review_score\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"promotion_flag\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","\n","    # NOTE: has to be done after aggregated_features_single_column\n","    data_for_training = data_for_training.sort_values(\"srch_id\")\n","\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_starrating\"\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_location_score2\"\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_location_score1\"\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_review_score\"\n","    )\n","\n","    gc.collect()\n","    if kind == \"train\":\n","        y = data_for_training[target_column].values\n","    else:\n","        y = None\n","\n","    training_set_only_metrics = [\"click_bool\", \"booking_bool\", \"gross_bookings_usd\"]\n","    columns_to_remove = [\n","        \"date_time\",\n","        \"target\",\n","        target_column,\n","    ] + training_set_only_metrics\n","    columns_to_remove = [\n","        c for c in columns_to_remove if c in data_for_training.columns.values\n","    ]\n","    data_for_training = data_for_training.drop(labels=columns_to_remove, axis=1)\n","    return data_for_training, y\n","\n","\n","def remove_columns(x1, ignore_column=[\"srch_id\", \"prop_id\", \"position\", \"random_bool\"]):\n","    ignore_column = [c for c in ignore_column if c in x1.columns.values]\n","    # print('Dropping columns: {}'.format(ignore_column))\n","    # ignore_column_numbers = [x1.columns.get_loc(x) for x in ignore_column]\n","    x1 = x1.drop(labels=ignore_column, axis=1)\n","    # print('Columns after dropping: {}'.format(x1.columns.values))\n","    return x1\n","\n","\n","def input_estimated_position(training_data, srch_id_dest_id_dict):\n","    training_data = training_data.merge(\n","        srch_id_dest_id_dict, how=\"left\", on=[\"srch_destination_id\", \"prop_id\"]\n","    )\n","    print(training_data.head())\n","    return training_data\n","\n","\n","def split_train_data(data_for_training, y, val_start=0, val_end=0):\n","\n","    x1 = pandas.concat([data_for_training[0:val_start], data_for_training[val_end:]])\n","    y1 = np.concatenate((y[0:val_start], y[val_end:]), axis=0)\n","    x2 = data_for_training[val_start:val_end]\n","    y2 = y[val_start:val_end]\n","\n","    srch_id_dest_id_dict = x1.loc[x1[\"random_bool\"] == 0]\n","\n","    # estimated position calculation\n","    srch_id_dest_id_dict = x1.loc[x1[\"random_bool\"] == 0]\n","    srch_id_dest_id_dict = x1.groupby([\"srch_destination_id\", \"prop_id\"]).agg(\n","        {\"position\": \"mean\"}\n","    )\n","    srch_id_dest_id_dict = srch_id_dest_id_dict.rename(\n","        index=str, columns={\"position\": \"estimated_position\"}\n","    ).reset_index()\n","    srch_id_dest_id_dict[\"srch_destination_id\"] = (\n","        srch_id_dest_id_dict[\"srch_destination_id\"].astype(str).astype(int)\n","    )\n","    srch_id_dest_id_dict[\"prop_id\"] = (\n","        srch_id_dest_id_dict[\"prop_id\"].astype(str).astype(int)\n","    )\n","    srch_id_dest_id_dict[\"estimated_position\"] = (\n","        1 / srch_id_dest_id_dict[\"estimated_position\"]\n","    )\n","    x1 = input_estimated_position(x1, srch_id_dest_id_dict)\n","    x2 = input_estimated_position(x2, srch_id_dest_id_dict)\n","\n","    groups = x1[\"srch_id\"].value_counts(sort=False).sort_index()\n","    eval_groups = x2[\"srch_id\"].value_counts(sort=False).sort_index()\n","    len(eval_groups), len(x2), len(x1), len(groups)\n","\n","    x1 = remove_columns(x1)\n","    x2 = remove_columns(x2)\n","    return (x1, x2, y1, y2, groups, eval_groups, srch_id_dest_id_dict)\n","\n","\n","def get_categorical_column(x1):\n","    categorical_features = [\n","        \"day\",\n","        \"month\",\n","        \"prop_country_id\",\n","        \"site_id\",\n","        \"visitor_location_country_id\",\n","    ]\n","    categorical_features = [c for c in categorical_features if c in x1.columns.values]\n","    categorical_features_numbers = [x1.columns.get_loc(x) for x in categorical_features]\n","    return categorical_features_numbers\n","\n","\n","def train_model(\n","    x1, x2, y1, y2, groups, eval_groups, lr, method, output_dir, name_of_model=None\n","):\n","    if not name_of_model:\n","        name_of_model = str(int(time.time()))\n","\n","    categorical_features_numbers = get_categorical_column(x1)\n","    clf = lightgbm.LGBMRanker(\n","        objective=\"lambdarank\",\n","        metric=\"ndcg\",\n","        n_estimators=100,\n","        learning_rate=lr,\n","        max_position=5,\n","        label_gain=[0, 1, 2],\n","        random_state=69,\n","        seed=69,\n","        boosting=method,\n","    )\n","    gc.collect()\n","\n","    print(\"Training on train set with columns: {}\".format(x1.columns.values))\n","    clf.fit(\n","        x1,\n","        y1,\n","        eval_set=[(x1, y1), (x2, y2)],\n","        eval_group=[groups, eval_groups],\n","        group=groups,\n","        eval_at=5,\n","        verbose=20,\n","        early_stopping_rounds=200,\n","        categorical_feature=categorical_features_numbers,\n","    )\n","    gc.collect()\n","    pickle.dump(clf, open(\"/content/drive/My Drive/Colab Notebooks/saved_model/model.dat\", \"wb\"))\n","    return clf\n","\n","\n","def predict(name_of_model, test_data, srch_id_dest_id_dict, output_dir):\n","\n","    gc.collect()\n","\n","    model = pickle.load(open(os.path.join(output_dir, \"model.dat\"), \"rb\"))\n","\n","    test_data = test_data.copy()\n","    test_data = input_estimated_position(test_data, srch_id_dest_id_dict)\n","\n","    test_data_srch_id_prop_id = test_data[[\"srch_id\", \"prop_id\"]]\n","\n","    test_data = remove_columns(test_data)\n","\n","    categorical_features_numbers = get_categorical_column(test_data)\n","\n","    print(\"Predicting on train set with columns: {}\".format(test_data.columns.values))\n","    kwargs = {}\n","    kwargs = {\"categorical_feature\": categorical_features_numbers}\n","\n","    predictions = model.predict(test_data, **kwargs)\n","    test_data_srch_id_prop_id[\"prediction\"] = predictions\n","    del test_data\n","    gc.collect()\n","\n","    test_data_srch_id_prop_id = test_data_srch_id_prop_id.sort_values(\n","        [\"srch_id\", \"prediction\"], ascending=False\n","    )\n","    print(\"Saving predictions into submission.csv\")\n","    test_data_srch_id_prop_id[[\"srch_id\", \"prop_id\"]].to_csv(\"/content/drive/My Drive/Colab Notebooks/submissions/submission.csv\", index=False)\n","\n","\n","def run(train_csv, test_csv, output_dir):\n","    name_of_model = str(int(time.time()))\n","\n","    training_data = load_data(train_csv)\n","    training_data, y = preprocess_training_data(training_data)\n","\n","    method = \"dart\"\n","    validation_num = 150000\n","    lr = 0.12\n","    training_data\n","    # for i in range(0, int(len(training_data.index) / validation_num)): # enable for cross-validation\n","    for i in range(0, 1):\n","        val_start = i * validation_num\n","        val_end = (i + 1) * validation_num\n","        x1, x2, y1, y2, groups, eval_groups, srch_id_dest_id_dict = split_train_data(\n","            training_data, y, val_start, val_end\n","        )\n","        pickle.dump(srch_id_dest_id_dict, open('/content/drive/My Drive/Colab Notebooks/srch_id_dest_id_dict.dat',\"wb\"))\n","        model = train_model(\n","            x1, x2, y1, y2, groups, eval_groups, lr, method, output_dir, name_of_model\n","        )\n","        # test_data = load_data(test_csv)  #Crashes.. Run seperately\n","        # test_data, _ = preprocess_training_data(test_data, kind=\"test\")\n","        # predict(name_of_model, test_data, srch_id_dest_id_dict, output_dir)\n","        # print(\"Submit the predictions file submission.csv to kaggle\")\n","\n","\n","train_csv = \"/content/drive/My Drive/Colab Notebooks/training_set_VU_DM.csv\"\n","test_csv = \"/content/drive/My Drive/Colab Notebooks/test_set_VU_DM.csv\"\n","output_dir = \"/content/drive/My Drive/Colab Notebooks/submissions\"\n","run(train_csv, test_csv, output_dir)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Started loading data from file /content/drive/My Drive/Colab Notebooks/training_set_VU_DM.csv\n","Finished loading data....\n","Preprocessing training data....\n","Dropping columns ['comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff', 'gross_bookings_usd'] because they miss more than 0.9 of data.\n","Dropped columns ['comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff', 'gross_bookings_usd']\n","   srch_id  site_id  ...  prop_review_score_norm_by_srch_id  estimated_position\n","0    10053        5  ...                           0.000000            0.049401\n","1    10053        5  ...                           0.992278            0.050967\n","2    10053        5  ...                           0.496139            0.137339\n","3    10053        5  ...                           0.496139            0.042135\n","4    10053        5  ...                          -0.992278            0.106707\n","\n","[5 rows x 60 columns]\n","   srch_id  site_id  ...  prop_review_score_norm_by_srch_id  estimated_position\n","0        1       12  ...                           0.016094            0.037859\n","1        1       12  ...                           0.917342            0.033208\n","2        1       12  ...                           0.016094            0.032661\n","3        1       12  ...                          -0.885154            0.034001\n","4        1       12  ...                           0.917342            0.049909\n","\n","[5 rows x 60 columns]\n","Training on train set with columns: ['site_id' 'visitor_location_country_id' 'visitor_hist_starrating'\n"," 'visitor_hist_adr_usd' 'prop_country_id' 'prop_starrating'\n"," 'prop_review_score' 'prop_brand_bool' 'prop_location_score1'\n"," 'prop_location_score2' 'prop_log_historical_price' 'price_usd'\n"," 'promotion_flag' 'srch_destination_id' 'srch_length_of_stay'\n"," 'srch_booking_window' 'srch_adults_count' 'srch_children_count'\n"," 'srch_room_count' 'srch_saturday_night_bool' 'srch_query_affinity_score'\n"," 'orig_destination_distance' 'comp2_rate' 'comp2_inv'\n"," 'comp2_rate_percent_diff' 'comp3_rate' 'comp3_inv' 'comp5_rate'\n"," 'comp5_inv' 'comp5_rate_percent_diff' 'comp8_rate' 'comp8_inv'\n"," 'comp8_rate_percent_diff' 'month' 'hour' 'dayofweek'\n"," 'price_usd_norm_by_srch_id' 'price_usd_norm_by_prop_id'\n"," 'prop_starrating_norm_by_srch_id' 'MEAN(prop_id, price_usd)'\n"," 'SUBSTRACT(price_usd, MEAN)' 'MEAN(srch_id, prop_starrating)'\n"," 'SUBSTRACT(prop_starrating, MEAN)' 'MEAN(srch_id, prop_location_score2)'\n"," 'SUBSTRACT(prop_location_score2, MEAN)'\n"," 'MEAN(srch_id, prop_location_score1)'\n"," 'SUBSTRACT(prop_location_score1, MEAN)'\n"," 'MEAN(srch_destination_id, price_usd)' 'MEAN(srch_id, prop_review_score)'\n"," 'SUBSTRACT(prop_review_score, MEAN)' 'MEAN(srch_id, promotion_flag)'\n"," 'SUBSTRACT(promotion_flag, MEAN)' 'prop_location_score2_norm_by_srch_id'\n"," 'prop_location_score1_norm_by_srch_id'\n"," 'prop_review_score_norm_by_srch_id' 'estimated_position']\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n","New categorical_feature is [0, 1, 4, 33]\n","  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n","/usr/local/lib/python3.6/dist-packages/lightgbm/callback.py:189: UserWarning: Early stopping is not available in dart mode\n","  warnings.warn('Early stopping is not available in dart mode')\n"],"name":"stderr"},{"output_type":"stream","text":["[20]\ttraining's ndcg@5: 0.38772\tvalid_1's ndcg@5: 0.371838\n","[40]\ttraining's ndcg@5: 0.393193\tvalid_1's ndcg@5: 0.379448\n","[60]\ttraining's ndcg@5: 0.397025\tvalid_1's ndcg@5: 0.382208\n","[80]\ttraining's ndcg@5: 0.398641\tvalid_1's ndcg@5: 0.383575\n","[100]\ttraining's ndcg@5: 0.401726\tvalid_1's ndcg@5: 0.385585\n","Started loading data from file /content/drive/My Drive/Colab Notebooks/test_set_VU_DM.csv\n","Finished loading data....\n","Preprocessing training data....\n","Dropping columns ['comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff'] because they miss more than 0.9 of data.\n","Dropped columns ['comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L5QwZVKsWnfN","colab_type":"code","colab":{}},"source":["import time\n","import argparse\n","import pickle\n","import os\n","import gc\n","\n","import pandas\n","import numpy as np\n","import lightgbm\n","\n","def load_data(file_path):\n","    gc.collect()\n","    print(\"Started loading data from file {}\".format(file_path))\n","    orig_data = pandas.read_csv(file_path)\n","    print(\"Finished loading data....\")\n","    return orig_data\n","\n","\n","def add_date_features(\n","    in_data, datetime_key=\"date_time\", features=[\"month\", \"hour\", \"dayofweek\"]\n","):\n","    dates = pandas.to_datetime(in_data[datetime_key])\n","    for feature in features:\n","        if feature == \"month\":\n","            in_data[\"month\"] = dates.dt.month\n","        elif feature == \"dayofweek\":\n","            in_data[\"dayofweek\"] = dates.dt.dayofweek\n","        elif feature == \"hour\":\n","            in_data[\"hour\"] = dates.dt.hour\n","\n","    return in_data\n","\n","\n","def normalize_features(input_df, group_key, target_column, take_log10=False):\n","\n","    # for numerical stability\n","    epsilon = 1e-4\n","    if take_log10:\n","        input_df[target_column] = np.log10(input_df[target_column] + epsilon)\n","    methods = [\"mean\", \"std\"]\n","\n","    df = input_df.groupby(group_key).agg({target_column: methods})\n","\n","    df.columns = df.columns.droplevel()\n","    col = {}\n","    for method in methods:\n","        col[method] = target_column + \"_\" + method\n","\n","    df.rename(columns=col, inplace=True)\n","    df_merge = input_df.merge(df.reset_index(), on=group_key)\n","    df_merge[target_column + \"_norm_by_\" + group_key] = (\n","        df_merge[target_column] - df_merge[target_column + \"_mean\"]\n","    ) / df_merge[target_column + \"_std\"]\n","    df_merge = df_merge.drop(labels=[col[\"mean\"], col[\"std\"]], axis=1)\n","\n","    gc.collect()\n","    return df_merge\n","\n","\n","# Add new columns to the dataframe\n","def aggregated_features_single_column(\n","    in_data,\n","    key_for_grouped_by=\"prop_id\",\n","    target_column=\"price_usd\",\n","    agg_methods=[\"mean\", \"median\", \"min\", \"max\"],\n","    transform_methods={\"mean\": [\"substract\"]},\n","):\n","    df = in_data.groupby(key_for_grouped_by).agg({target_column: agg_methods})\n","\n","    if isinstance(key_for_grouped_by, list):\n","        str_key_for_grouped_by = \"|\".join(key_for_grouped_by)\n","    else:\n","        str_key_for_grouped_by = key_for_grouped_by\n","\n","    df.columns = df.columns.droplevel()\n","    col = {}\n","    for method in agg_methods:\n","        col[method] = (\n","            method.upper() + \"(\" + str_key_for_grouped_by + \", \" + target_column + \")\"\n","        )\n","\n","    df.rename(columns=col, inplace=True)\n","\n","    in_data = in_data.merge(df.reset_index(), on=key_for_grouped_by)\n","    for method_name in transform_methods:\n","        for applying_function in transform_methods[method_name]:\n","            function_data = in_data[\n","                method_name.upper()\n","                + \"(\"\n","                + str_key_for_grouped_by\n","                + \", \"\n","                + target_column\n","                + \")\"\n","            ]\n","            column_data = in_data[target_column]\n","            if applying_function == \"substract\":\n","                result = column_data - function_data\n","            elif applying_function == \"divide\":\n","                result = column_data / function_data\n","            else:\n","                continue\n","            in_data[\n","                applying_function.upper()\n","                + \"(\"\n","                + target_column\n","                + \", \"\n","                + method_name.upper()\n","                + \")\"\n","            ] = result\n","    gc.collect()\n","\n","    return in_data\n","\n","\n","def drop_columns_with_missing_data(\n","    df,\n","    threshold,\n","    ignore_values=[\n","        \"visitor_hist_adr_usd\",\n","        \"visitor_hist_starrating\",\n","        \"srch_query_affinity_score\",\n","    ],\n","):\n","    columns_to_drop = []\n","\n","    for i in range(df.shape[1]):\n","        length_df = len(df)\n","        column_names = df.columns.tolist()\n","        number_nans = sum(df.iloc[:, i].isnull())\n","        if number_nans / length_df > threshold:\n","            if column_names[i] not in ignore_values:\n","                columns_to_drop.append(column_names[i])\n","\n","    print(\n","        \"Dropping columns {} because they miss more than {} of data.\".format(\n","            columns_to_drop, threshold\n","        )\n","    )\n","\n","    df_reduced = df.drop(labels=columns_to_drop, axis=1)\n","    print(\"Dropped columns {}\".format(columns_to_drop))\n","    return df_reduced\n","\n","\n","def preprocess_training_data(orig_data, kind=\"train\", use_ndcg_choices=False):\n","\n","    print(\"Preprocessing training data....\")\n","    gc.collect()\n","    data_for_training = orig_data\n","\n","    target_column = \"target\"\n","\n","    if kind == \"train\":\n","        conditions = [\n","            data_for_training[\"click_bool\"] == 1,\n","            data_for_training[\"booking_bool\"] == 1,\n","        ]\n","        choices = [1, 2]\n","        data_for_training[target_column] = np.select(conditions, choices, default=0)\n","\n","    threshold = 0.9\n","    data_for_training = add_date_features(data_for_training)\n","    data_for_training.drop(labels=[\"date_time\"], axis=1, inplace=True)\n","\n","    data_for_training = drop_columns_with_missing_data(data_for_training, threshold)\n","\n","    # do not normalize 2 times with take_log10\n","    data_for_training = normalize_features(\n","        data_for_training,\n","        group_key=\"srch_id\",\n","        target_column=\"price_usd\",\n","        take_log10=True,\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"prop_id\", target_column=\"price_usd\"\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_starrating\"\n","    )\n","\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training, \"prop_id\", \"price_usd\", [\"mean\"]\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"prop_starrating\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"prop_location_score2\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"prop_location_score1\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_destination_id\",\n","        target_column=\"price_usd\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"prop_review_score\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","    data_for_training = aggregated_features_single_column(\n","        data_for_training,\n","        key_for_grouped_by=\"srch_id\",\n","        target_column=\"promotion_flag\",\n","        agg_methods=[\"mean\"],\n","        transform_methods={\"mean\": [\"substract\"]},\n","    )\n","\n","    # NOTE: has to be done after aggregated_features_single_column\n","    data_for_training = data_for_training.sort_values(\"srch_id\")\n","\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_starrating\"\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_location_score2\"\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_location_score1\"\n","    )\n","    data_for_training = normalize_features(\n","        data_for_training, group_key=\"srch_id\", target_column=\"prop_review_score\"\n","    )\n","\n","    gc.collect()\n","    if kind == \"train\":\n","        y = data_for_training[target_column].values\n","    else:\n","        y = None\n","\n","    training_set_only_metrics = [\"click_bool\", \"booking_bool\", \"gross_bookings_usd\"]\n","    columns_to_remove = [\n","        \"date_time\",\n","        \"target\",\n","        target_column,\n","    ] + training_set_only_metrics\n","    columns_to_remove = [\n","        c for c in columns_to_remove if c in data_for_training.columns.values\n","    ]\n","    data_for_training = data_for_training.drop(labels=columns_to_remove, axis=1)\n","    return data_for_training, y\n","\n","\n","def remove_columns(x1, ignore_column=[\"srch_id\", \"prop_id\", \"position\", \"random_bool\"]):\n","    ignore_column = [c for c in ignore_column if c in x1.columns.values]\n","    # print('Dropping columns: {}'.format(ignore_column))\n","    # ignore_column_numbers = [x1.columns.get_loc(x) for x in ignore_column]\n","    x1 = x1.drop(labels=ignore_column, axis=1)\n","    # print('Columns after dropping: {}'.format(x1.columns.values))\n","    return x1\n","\n","\n","def input_estimated_position(training_data, srch_id_dest_id_dict):\n","    training_data = training_data.merge(\n","        srch_id_dest_id_dict, how=\"left\", on=[\"srch_destination_id\", \"prop_id\"]\n","    )\n","    print(training_data.head())\n","    return training_data\n","\n","\n","def split_train_data(data_for_training, y, val_start=0, val_end=0):\n","\n","    x1 = pandas.concat([data_for_training[0:val_start], data_for_training[val_end:]])\n","    y1 = np.concatenate((y[0:val_start], y[val_end:]), axis=0)\n","    x2 = data_for_training[val_start:val_end]\n","    y2 = y[val_start:val_end]\n","\n","    srch_id_dest_id_dict = x1.loc[x1[\"random_bool\"] == 0]\n","\n","    # estimated position calculation\n","    srch_id_dest_id_dict = x1.loc[x1[\"random_bool\"] == 0]\n","    srch_id_dest_id_dict = x1.groupby([\"srch_destination_id\", \"prop_id\"]).agg(\n","        {\"position\": \"mean\"}\n","    )\n","    srch_id_dest_id_dict = srch_id_dest_id_dict.rename(\n","        index=str, columns={\"position\": \"estimated_position\"}\n","    ).reset_index()\n","    srch_id_dest_id_dict[\"srch_destination_id\"] = (\n","        srch_id_dest_id_dict[\"srch_destination_id\"].astype(str).astype(int)\n","    )\n","    srch_id_dest_id_dict[\"prop_id\"] = (\n","        srch_id_dest_id_dict[\"prop_id\"].astype(str).astype(int)\n","    )\n","    srch_id_dest_id_dict[\"estimated_position\"] = (\n","        1 / srch_id_dest_id_dict[\"estimated_position\"]\n","    )\n","    x1 = input_estimated_position(x1, srch_id_dest_id_dict)\n","    x2 = input_estimated_position(x2, srch_id_dest_id_dict)\n","\n","    groups = x1[\"srch_id\"].value_counts(sort=False).sort_index()\n","    eval_groups = x2[\"srch_id\"].value_counts(sort=False).sort_index()\n","    len(eval_groups), len(x2), len(x1), len(groups)\n","\n","    x1 = remove_columns(x1)\n","    x2 = remove_columns(x2)\n","    return (x1, x2, y1, y2, groups, eval_groups, srch_id_dest_id_dict)\n","\n","\n","def get_categorical_column(x1):\n","    categorical_features = [\n","        \"day\",\n","        \"month\",\n","        \"prop_country_id\",\n","        \"site_id\",\n","        \"visitor_location_country_id\",\n","    ]\n","    categorical_features = [c for c in categorical_features if c in x1.columns.values]\n","    categorical_features_numbers = [x1.columns.get_loc(x) for x in categorical_features]\n","    return categorical_features_numbers\n","\n","\n","def train_model(\n","    x1, x2, y1, y2, groups, eval_groups, lr, method, output_dir, name_of_model=None\n","):\n","    if not name_of_model:\n","        name_of_model = str(int(time.time()))\n","\n","    categorical_features_numbers = get_categorical_column(x1)\n","    clf = lightgbm.LGBMRanker(\n","        objective=\"lambdarank\",\n","        metric=\"ndcg\",\n","        n_estimators=100,\n","        learning_rate=lr,\n","        max_position=5,\n","        label_gain=[0, 1, 2],\n","        random_state=69,\n","        seed=69,\n","        boosting=method,\n","    )\n","    gc.collect()\n","\n","    print(\"Training on train set with columns: {}\".format(x1.columns.values))\n","    clf.fit(\n","        x1,\n","        y1,\n","        eval_set=[(x1, y1), (x2, y2)],\n","        eval_group=[groups, eval_groups],\n","        group=groups,\n","        eval_at=5,\n","        verbose=20,\n","        early_stopping_rounds=200,\n","        categorical_feature=categorical_features_numbers,\n","    )\n","    gc.collect()\n","    pickle.dump(clf, open(\"/content/drive/My Drive/Colab Notebooks/saved_model/model.dat\", \"wb\"))\n","    return clf\n","\n","\n","def predict(name_of_model, test_data, srch_id_dest_id_dict, output_dir):\n","\n","    gc.collect()\n","\n","    model = pickle.load(open(os.path.join(output_dir, \"model.dat\"), \"rb\"))\n","\n","    test_data = test_data.copy()\n","    test_data = input_estimated_position(test_data, srch_id_dest_id_dict)\n","\n","    test_data_srch_id_prop_id = test_data[[\"srch_id\", \"prop_id\"]]\n","\n","    test_data = remove_columns(test_data)\n","\n","    categorical_features_numbers = get_categorical_column(test_data)\n","\n","    print(\"Predicting on train set with columns: {}\".format(test_data.columns.values))\n","    kwargs = {}\n","    kwargs = {\"categorical_feature\": categorical_features_numbers}\n","\n","    predictions = model.predict(test_data, **kwargs)\n","    test_data_srch_id_prop_id[\"prediction\"] = predictions\n","    del test_data\n","    gc.collect()\n","\n","    test_data_srch_id_prop_id = test_data_srch_id_prop_id.sort_values(\n","        [\"srch_id\", \"prediction\"], ascending=False\n","    )\n","    print(\"Saving predictions into submission.csv\")\n","    test_data_srch_id_prop_id[[\"srch_id\", \"prop_id\"]].to_csv(\"/content/drive/My Drive/Colab Notebooks/submissions/submission.csv\", index=False)\n","\n","\n","def run(train_csv, test_csv, output_dir):\n","    name_of_model = str(int(time.time()))\n","\n","    training_data = load_data(train_csv)\n","    training_data, y = preprocess_training_data(training_data)\n","\n","    method = \"dart\"\n","    validation_num = 150000\n","    lr = 0.12\n","    # for i in range(0, int(len(training_data.index) / validation_num)): # enable for cross-validation\n","    for i in range(0, 1):\n","        val_start = i * validation_num\n","        val_end = (i + 1) * validation_num\n","        x1, x2, y1, y2, groups, eval_groups, srch_id_dest_id_dict = split_train_data(\n","            training_data, y, val_start, val_end\n","        )\n","        np.save('/content/drive/My Drive/Colab Notebooks/srch_id_dest_id_dict.npy', srch_id_dest_id_dict)\n","        model = train_model(\n","            x1, x2, y1, y2, groups, eval_groups, lr, method, output_dir, name_of_model\n","        )\n","        test_data = load_data(test_csv)\n","        test_data, _ = preprocess_training_data(test_data, kind=\"test\")\n","        predict(name_of_model, test_data, srch_id_dest_id_dict, output_dir)\n","        print(\"Submit the predictions file submission.csv to kaggle\")\n","\n","load_lr_model = pickle.load(open(\"/content/drive/My Drive/Colab Notebooks/saved_model/model.dat\", 'rb'))\n","srch_id_dest_id_dict = pickle.load(open('/content/drive/My Drive/Colab Notebooks/srch_id_dest_id_dict.dat','rb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8kaLEL0c34D","colab_type":"code","outputId":"ea91f5fd-6e56-4892-b196-cdea670fce62","executionInfo":{"status":"ok","timestamp":1590085988478,"user_tz":-120,"elapsed":595,"user":{"displayName":"S. NEERAJ S. NEERAJ","photoUrl":"","userId":"07436124774494631839"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["print(srch_id_dest_id_dict[\"srch_destination_id\"])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0             2\n","1             2\n","2             2\n","3             2\n","4             2\n","          ...  \n","602877    28416\n","602878    28416\n","602879    28416\n","602880    28416\n","602881    28416\n","Name: srch_destination_id, Length: 602882, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_37W9f3rXVaU","colab_type":"code","outputId":"5b993798-72ee-43cd-eebf-c815c26d956a","executionInfo":{"status":"error","timestamp":1590086209191,"user_tz":-120,"elapsed":170879,"user":{"displayName":"S. NEERAJ S. NEERAJ","photoUrl":"","userId":"07436124774494631839"}},"colab":{"base_uri":"https://localhost:8080/","height":319}},"source":["train_csv = \"/content/drive/My Drive/Colab Notebooks/training_set_VU_DM.csv\"\n","test_csv = \"/content/drive/My Drive/Colab Notebooks/test_set_VU_DM.csv\"\n","output_dir = \"/content/drive/My Drive/Colab Notebooks/submissions\"\n","test_data = load_data(test_csv)\n","test_data, _ = preprocess_training_data(test_data, kind=\"test\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Started loading data from file /content/drive/My Drive/Colab Notebooks/test_set_VU_DM.csv\n","Finished loading data....\n","Preprocessing training data....\n","Dropping columns ['comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff'] because they miss more than 0.9 of data.\n","Dropped columns ['comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff']\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-db3cebf2c89d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_of_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrch_id_dest_id_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Submit the predictions file submission.csv to kaggle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'name_of_model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"lNdFs6UShrpt","colab_type":"code","outputId":"2425fdcf-b062-48d6-bbfc-f39fe9bafe99","executionInfo":{"status":"ok","timestamp":1590086442993,"user_tz":-120,"elapsed":51071,"user":{"displayName":"S. NEERAJ S. NEERAJ","photoUrl":"","userId":"07436124774494631839"}},"colab":{"base_uri":"https://localhost:8080/","height":578}},"source":["def predict(model, test_data, srch_id_dest_id_dict, output_dir):\n","\n","    gc.collect()\n","\n","    test_data = test_data.copy()\n","    test_data = input_estimated_position(test_data, srch_id_dest_id_dict)\n","\n","    test_data_srch_id_prop_id = test_data[[\"srch_id\", \"prop_id\"]]\n","\n","    test_data = remove_columns(test_data)\n","\n","    categorical_features_numbers = get_categorical_column(test_data)\n","\n","    print(\"Predicting on train set with columns: {}\".format(test_data.columns.values))\n","    kwargs = {}\n","    kwargs = {\"categorical_feature\": categorical_features_numbers}\n","\n","    predictions = model.predict(test_data, **kwargs)\n","    test_data_srch_id_prop_id[\"prediction\"] = predictions\n","    del test_data\n","    gc.collect()\n","\n","    test_data_srch_id_prop_id = test_data_srch_id_prop_id.sort_values(\n","        [\"srch_id\", \"prediction\"], ascending=False\n","    )\n","    print(\"Saving predictions into submission.csv\")\n","    test_data_srch_id_prop_id[[\"srch_id\", \"prop_id\"]].to_csv(\"/content/drive/My Drive/Colab Notebooks/submissions/submission.csv\", index=False)\n","    \n","predict(load_lr_model, test_data, srch_id_dest_id_dict, output_dir)\n","print(\"Submit the predictions file submission.csv to kaggle\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   srch_id  site_id  ...  prop_review_score_norm_by_srch_id  estimated_position\n","0        1       24  ...                           0.513567            0.066122\n","1        1       24  ...                           0.513567            0.056417\n","2        1       24  ...                           1.045476            0.036213\n","3        1       24  ...                          -1.082159            0.034177\n","4        1       24  ...                          -1.082159            0.071546\n","\n","[5 rows x 59 columns]\n","Predicting on train set with columns: ['site_id' 'visitor_location_country_id' 'visitor_hist_starrating'\n"," 'visitor_hist_adr_usd' 'prop_country_id' 'prop_starrating'\n"," 'prop_review_score' 'prop_brand_bool' 'prop_location_score1'\n"," 'prop_location_score2' 'prop_log_historical_price' 'price_usd'\n"," 'promotion_flag' 'srch_destination_id' 'srch_length_of_stay'\n"," 'srch_booking_window' 'srch_adults_count' 'srch_children_count'\n"," 'srch_room_count' 'srch_saturday_night_bool' 'srch_query_affinity_score'\n"," 'orig_destination_distance' 'comp2_rate' 'comp2_inv'\n"," 'comp2_rate_percent_diff' 'comp3_rate' 'comp3_inv' 'comp5_rate'\n"," 'comp5_inv' 'comp5_rate_percent_diff' 'comp8_rate' 'comp8_inv'\n"," 'comp8_rate_percent_diff' 'month' 'hour' 'dayofweek'\n"," 'price_usd_norm_by_srch_id' 'price_usd_norm_by_prop_id'\n"," 'prop_starrating_norm_by_srch_id' 'MEAN(prop_id, price_usd)'\n"," 'SUBSTRACT(price_usd, MEAN)' 'MEAN(srch_id, prop_starrating)'\n"," 'SUBSTRACT(prop_starrating, MEAN)' 'MEAN(srch_id, prop_location_score2)'\n"," 'SUBSTRACT(prop_location_score2, MEAN)'\n"," 'MEAN(srch_id, prop_location_score1)'\n"," 'SUBSTRACT(prop_location_score1, MEAN)'\n"," 'MEAN(srch_destination_id, price_usd)' 'MEAN(srch_id, prop_review_score)'\n"," 'SUBSTRACT(prop_review_score, MEAN)' 'MEAN(srch_id, promotion_flag)'\n"," 'SUBSTRACT(promotion_flag, MEAN)' 'prop_location_score2_norm_by_srch_id'\n"," 'prop_location_score1_norm_by_srch_id'\n"," 'prop_review_score_norm_by_srch_id' 'estimated_position']\n","Saving predictions into submission.csv\n","Submit the predictions file submission.csv to kaggle\n"],"name":"stdout"}]}]}